---
title: "ETC3555/5555 2022 - Lab 7"
subtitle: "Deep neural networks"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
rm(list=ls())
```


# Preliminaries

We will be working with the Keras framework to implement our deep neural network (DNN). Keras is a front end for several DNN frameworks (e.g. TensorFlow, Theano, etc.) which aims to simplify the coding process. Install Keras for `R` by running:

```{r echo = TRUE, include = TRUE, eval = FALSE}
install.packages("keras")
install.packages("tensorflow")
library(tensorflow)
install_tensorflow(method = "auto")

library(keras)
install_keras(method = c("conda"), conda = "auto", tensorflow = "default",
              extra_packages = c("tensorflow-hub"))
```

TensorFlow should automatically be installed and selected as the default backend.

If at any stage you need to retrain your models from scratch make sure to clear your workspace and refresh your R session. Failing to do so will result in continued training of existing models or unexpected errors.

# Exercises


## Exercise 1

(a) Softmax converts to a probability. Relu is just an efficient way to introduce non-linearities into the neural network model. It tends to outperform other activation functions.
(b) Maybe - what did you find?
(c) From [FAQ](https://keras.io/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss):

>> Why is the training loss much higher than the testing loss?

>> A Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time.

>> Besides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.

(d) Dropout is useful when training as a form of regularization. You don't want to apply dropout during the final predictions as it will decrease accuracy.
(e) Overfitting.


## Exercise 2



```{r}
library(tidyverse)
theme_set(theme_bw())
library(keras)
library(rsample)

n_epochs <- 2

mnist <- dataset_mnist()

x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

# reshape
dim(x_train) <- c(nrow(x_train), 784)
dim(x_test) <- c(nrow(x_test), 784)


# rescale
x_train <- x_train / 255
x_test <- x_test / 255

y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)


# Initialise list
model_list <- list()

# Exercise 1
model_list[["Tutorial"]]$model <- keras_model_sequential() %>%
  layer_dense(units = 256, activation = "relu", input_shape = c(784)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = "softmax") %>%
  compile(loss = "categorical_crossentropy",
          optimizer = optimizer_rmsprop(),
          metrics = c("accuracy"))

model_list[["Tutorial"]]$fit <- model_list[["Tutorial"]]$model %>%
  fit(x_train, y_train, epochs = n_epochs, batch_size = 128,
      validation_split = 0.2)

# Assignment question 1 (a)
model_list[["Dropout"]]$model <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = c(784)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = "softmax") %>%
  compile(loss = "categorical_crossentropy",
          optimizer = optimizer_rmsprop(),
          metrics = c("accuracy"))

model_list[["Dropout"]]$fit <- model_list[["Dropout"]]$model %>%
  fit(x_train, y_train, epochs = n_epochs, batch_size = 128,
      validation_split = 0.2)

# Assignment question 1 (b)
model_list[["L1"]]$model <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = c(784),
              kernel_regularizer = regularizer_l1(1e-4)) %>%
  layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l1(1e-4)) %>%
  layer_dense(units = 10, activation = "softmax") %>%
  compile(loss = "categorical_crossentropy",
          optimizer = optimizer_rmsprop(),
          metrics = c("accuracy"))

model_list[["L1"]]$fit <- model_list[["L1"]]$model %>%
  fit(x_train, y_train, epochs = n_epochs, batch_size = 128,
      validation_split = 0.2)

# Assignment question 1 (c)
model_list[["Early stopping"]]$model <- keras_model_sequential() %>%
  layer_dense(units = 128, activation = "relu", input_shape = c(784)) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 10, activation = "softmax") %>%
  compile(loss = "categorical_crossentropy",
          optimizer = optimizer_rmsprop(),
          metrics = c("accuracy"))

model_list[["Early stopping"]]$fit <- model_list[["Early stopping"]]$model %>%
  fit(x_train, y_train, epochs = n_epochs, batch_size = 128,
      validation_split = 0.2,
      callbacks = callback_early_stopping(monitor = 'val_loss',
                                          patience = 2))


# Evaluate models
evaluate_df <- function(model) {
  evaluate(model, x_test, y_test, verbose = 0) %>%
    bind_rows()
}

# convert list to data frame for easier processing
model_df <- data_frame(
  name = fct_inorder(names(model_list)), # use factor for plot order
  model = map(model_list, "model"),  # pull out model element
  fit = map(model_list, "fit")  # pull out fit element
) %>%
  mutate(eval = map(model, evaluate_df)) %>%
  unnest(eval)

k_clear_session()

# Plots - alternative commented out below
# walk2(model_df$fit, model_df$name,
#       function(fit, name) {
#         # plot method for keras_training_history object returns ggplot object
#         p <- plot(fit) +
#           ggtitle(name)
#         print(p)
#       })

model_df %>%
  mutate(metrics = map(fit, function(x) { data.frame(x) })) %>%
  select(name, metrics) %>%
  unnest(metrics) %>%
  ggplot(aes(x = epoch, y = value, colour = data)) +
  geom_line() +
  facet_grid(metric~name, scales = "free_y")

model_df %>%
  select(name, loss, accuracy) %>% 
  knitr::kable(caption = "Loss and accuracy for Keras models")

model_df %>%
  select(name, loss, accuracy) %>%
  gather(var, val, -name) %>%
  ggplot(aes(x = name, y = val)) +
  geom_col(colour = "blue", fill = "blue", alpha = 0.3, width = 0) +
  geom_point(colour = "blue") +
  facet_wrap(~var, scales = "free_x") +
  coord_flip()
```

##  Exercise 3

(4 marks)

```{r}
# Adapted from https://tidymodels.github.io/rsample/articles/Applications/Keras.html

#### Helper functions ---------------------------------------------------------

evaluate_split <- function(units, dropout_rate, split, ...) {
  cat("Fitting: split = ", split$id[[1]], "; units = ", units, "; dropout = ",
      dropout_rate, "\n", sep = "")
  set.seed(4109)
  on.exit(keras::backend()$clear_session())

  # Define a single layer NN with dropout
  model <- keras_model_sequential() %>%
    layer_dense(units = units, activation = "relu", input_shape = c(784)) %>%
    layer_dropout(rate = dropout_rate) %>%
    layer_dense(units = 10, activation = "softmax") %>%
    compile(loss = "categorical_crossentropy",
            optimizer = optimizer_rmsprop(),
            metrics = c("accuracy"))
  
  # The data used for modeling (aka the "analysis" set)
  # Note: need to convert back to matrices for keras
  x_train_split <- analysis(split) %>%
    select(starts_with("X")) %>%
    as.matrix()
  
  y_train_split <- analysis(split) %>%
    select(starts_with("Y")) %>%
    as.matrix()

  # Fit to training (analysis) set
  fit(model, x = x_train_split, y = y_train_split, ...)
  
  # Now obtain the holdout set for evaluation
  x_test_split <- assessment(split) %>%
    select(starts_with("X")) %>%
    as.matrix()
  
  y_test_split <- assessment(split) %>%
    select(starts_with("Y")) %>%
    as.matrix()
  
  results <- evaluate(model, x_test_split, y_test_split, verbose = 0)
  results[1]

  
}


across_grid <- function(split, grid_df, ...) {
  # Execute grid for this resample
  grid_df$loss <- pmap_dbl(
    grid_df,
    evaluate_split,
    split = split,
    ...  # extra options for `fit`
  )

  grid_df$id <- split$id[[1]]
  
  grid_df
}


#### Training -----------------------------------------------------------------

# Convert matrices to dataframes for easier (and quicker) resampling. Need to 
# be a bit fancy because y is a matrix of values (not just a single response).
x_train_df <- as_data_frame(x_train)
names(x_train_df) <- paste0("X", 1:ncol(x_train_df))
y_train_df <- as_data_frame(y_train)
names(y_train_df) <- paste0("Y", 1:ncol(y_train_df))
mnist_train <- bind_cols(x_train_df, y_train_df)

# 5-fold cross validation
cv_splits <- vfold_cv(mnist_train, v = 2)

# Create grid
grid_df <- list(units = c(16, 32),
                dropout_rate = c(0, .25)) %>%
  cross_df()

# Create grid
#grid_df <- list(units = c(16, 32, 64, 128, 256, 512, 1024, 2048),
#                dropout_rate = c(0, .25, .5, .75)) %>%
#  cross_df()

# Fit model to each fold and hyperparameter combination
if (file.exists("tune_results.RData")) {
  load("tune_results.RData")
} else {
  tune_results <- map_df(
    cv_splits$splits,
    across_grid,
    grid_df = grid_df,
    batch_size = 128,
    epochs = n_epochs,
    verbose = 1,
    view_metrics = 0
  )
  
  #dir.create("cache", F, T)
  
  save(tune_results, file = "tune_results.RData")
}

plot_heatmap <- function(df) {
  ggplot(df, aes(x = units, y = dropout_rate)) + 
    geom_tile(aes(fill = loss)) +
    geom_tile(aes(colour = optimal, size = optimal), fill = NA) +
    geom_text(aes(label = round(loss, 2)), colour = "white") +
    scale_x_log10(breaks = unique(grid_df$units)) +
    scale_fill_gradient(high = "#132B43", low = "#56B1F7") +
    scale_size_manual("optimal", values=c(0, 1)) + 
    scale_colour_manual("optimal", values = c(NA, "darkorange1")) +
    labs(title = "Grid search for optimal dropout rate and number of units",
         subtitle = "Based on 5-fold cross validation")
}
```

We can see that the optimal performance using 5-fold cross validation on our training set occurs at the boundary of our subset of the hyperparameter space. If we increase the size of our hyperparameter space we observe a better combination of values.

```{r}
# Check average performance for units given in question
tune_results %>% 
  filter(units %in% c(16, 32, 64, 128)) %>% 
  group_by(units, dropout_rate) %>% 
  summarise(loss = mean(loss)) %>% 
  ungroup() %>% 
  mutate(optimal = loss == min(loss)) %>% 
  plot_heatmap()
```

```{r}
# Check average performance for more units
tune_results %>% 
  group_by(units, dropout_rate) %>% 
  summarise(loss = mean(loss)) %>% 
  ungroup() %>% 
  mutate(optimal = loss == min(loss)) %>% 
  plot_heatmap()
```


##  Exercise 4

This is just a matter of updating the `optimizer` argument in `compile`. For example:

```{r echo = TRUE, include = TRUE, eval = FALSE}
compile(loss = "categorical_crossentropy",
        optimizer = optimizer_sgd(momentum = 0.3),
        metrics = c("accuracy"))
```

You'll likely see that momentum is fastest, but doesn't reach the same accuracy. To reach same accuracy it could run for more epochs, but then might not be the fastest anymore. RMSProp and ADAM reach a given accuracy (say 95%) in fewer epochs. You can use `system.time()` or `microbenchmark` from the `microbenchmark` package to time how long each takes.
