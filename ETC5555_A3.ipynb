{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1peJaczc0nMwEZh0zoJZzEiiJsSFJDoH0",
      "authorship_tag": "ABX9TyM5V3zYI0mnW/w3ePhQ+gbN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahinya17/Neural-Network-with-Keras/blob/master/ETC5555_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xhVJjYBn_PgO",
        "outputId": "fc277a26-b79e-42bb-dda4-9bdcbf70d588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The rpy2.ipython extension is already loaded. To reload it, use:\n",
            "  %reload_ext rpy2.ipython\n"
          ]
        }
      ],
      "source": [
        "%load_ext rpy2.ipython"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data and create word embeddings"
      ],
      "metadata": {
        "id": "T3sxS_tz_cz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "lines <- readLines(\"glove.6B.100d.txt\")\n",
        "embeddings_index <- new.env(hash = TRUE, parent = emptyenv())\n",
        "for (i in 1:length(lines)) {\n",
        "  line <- lines[[i]]\n",
        "  values <- strsplit(line, \" \")[[1]]\n",
        "  word <- values[[1]]\n",
        "  embeddings_index[[word]] <- as.double(values[-1])\n",
        "}"
      ],
      "metadata": {
        "id": "TT3-yHPTPjDd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Write a function that calculates the cosine similarity between 2 vector representation of 2 words**"
      ],
      "metadata": {
        "id": "s1S8vExEDFyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "cosine_similarity <- function(word1_vec, word2_vec){\n",
        "    \n",
        "    cosine_similarity = (word1_vec %*% word2_vec) / \n",
        "    (sqrt(sum(word1_vec**2)) * sqrt(sum(word2_vec**2)))\n",
        "    \n",
        "    return(cosine_similarity)\n",
        "}"
      ],
      "metadata": {
        "id": "LqPJwu8PQta5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Calculate the cosine similarity between:**"
      ],
      "metadata": {
        "id": "qHwU9t-gDPQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# a. “king” and “queen”\n",
        "king_queen = cosine_similarity(embeddings_index[['king']], \n",
        "                               embeddings_index[['queen']])\n",
        "\n",
        "# b. \"king\" and \"potato\"\n",
        "king_potato = cosine_similarity(embeddings_index[['king']], \n",
        "                                embeddings_index[['potato']])\n",
        "\n",
        "# c. Pick another 2 words and comment if the similarities seem plausible\n",
        "plant_chair = cosine_similarity(embeddings_index[['plant']], \n",
        "                                embeddings_index[['chair']])\n",
        "plant_leaf = cosine_similarity(embeddings_index[['plant']], \n",
        "                               embeddings_index[['leaf']])\n",
        "\n",
        "print(paste0(\"king + queen: \", king_queen))\n",
        "print(paste0(\"king + potato: \", king_potato))\n",
        "print(paste0(\"plant + chair: \", plant_chair))\n",
        "print(paste0(\"plant + leaf: \", plant_leaf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gLnA4HEfSotT",
        "outputId": "663d38aa-2094-44c7-caea-103cc8878ac9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"king + queen: 0.750769079362385\"\n",
            "[1] \"king + potato: 0.184115797054578\"\n",
            "[1] \"plant + chair: 0.133337478808709\"\n",
            "[1] \"plant + leaf: 0.513181511864638\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can be observed, words like king and queen and plant and leaf higher word similarities when compared to king and potato and plant and leaf\n",
        "\n",
        "## **3. Write a function that takes as argument a word embedding vector and finds the closest word within the word embedding. The distance metric is defined again as cosine similarity.**\n"
      ],
      "metadata": {
        "id": "_ZhzKsnUDu8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "find_closest <- function(embeddings_index, word_vec, word_list){\n",
        "    # Maximum similarity threshold is set to a low value so that \n",
        "    # the comparison improves this value\n",
        "    similarity = -10000\n",
        "\n",
        "    # Similar word is initially empty\n",
        "    similar_word = ''\n",
        "\n",
        "    # Loop through the word embeddings\n",
        "    for (embed_word in ls(embeddings_index)){\n",
        "        \n",
        "        # ensure the word is not part of the inpt word list\n",
        "        if (!embed_word %in% word_list){\n",
        "            \n",
        "            # Get similarity of the 2 words\n",
        "            current_similarity = cosine_similarity(word_vec, \n",
        "                                                embeddings_index[[embed_word]])\n",
        "\n",
        "            # Check If the current similarity is greater than the \n",
        "            # previously set similarity\n",
        "            if (current_similarity > similarity){\n",
        "                \n",
        "                # Replace maximum similarity to be the current similarity\n",
        "                similarity = current_similarity\n",
        "\n",
        "                # Set the most similar word to be the current word\n",
        "                similar_word = embed_word\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return (c(similar_word, similarity))\n",
        "}"
      ],
      "metadata": {
        "id": "zoX54qoKhxPS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Calculate the following word embedding operation, using the function you wrote. Comment on the possible bias the result represents.**"
      ],
      "metadata": {
        "id": "HJObStd5FGEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# a. woman - man + king\n",
        "\n",
        "# Perform operation\n",
        "word_embed = embeddings_index[['woman']] - \n",
        "              embeddings_index[['man']] + \n",
        "              embeddings_index[['king']]\n",
        "\n",
        "# Get closest word and similarity\n",
        "closest = find_closest(embeddings_index, \n",
        "                       word_embed, c('woman', 'man', 'king'))\n",
        "\n",
        "# Print the closest word and similarity\n",
        "print(paste0(\"Closest word to woman - man + king: \", closest[1]))\n",
        "print(paste0(\"Cosine similarity: \", closest[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DwKWnPBVjbq5",
        "outputId": "b1ea4fc2-b46d-4a5d-ea74-356b9f387f45"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Closest word to woman - man + king: queen\"\n",
            "[1] \"Cosine similarity: 0.783441433588146\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# b. female - male + doctor\n",
        "\n",
        "# Perform operation\n",
        "word_embed = embeddings_index[['female']] - \n",
        "              embeddings_index[['male']] + \n",
        "              embeddings_index[['doctor']]\n",
        "\n",
        "# Get closest word and similarity\n",
        "closest = find_closest(embeddings_index, \n",
        "                       word_embed, c('female', 'male', 'doctor'))\n",
        "\n",
        "# Print the closest word and similarity\n",
        "print(paste0(\"Closest word to female - male + doctor: \", closest[1]))\n",
        "print(paste0(\"Cosine similarity: \", closest[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kfpVvpbqlN9r",
        "outputId": "4966320b-d6e4-49db-c15f-30c8dbb42ceb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Closest word to female - male + doctor: nurse\"\n",
            "[1] \"Cosine similarity: 0.749741727760762\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# c. female - male + programmer\n",
        "\n",
        "# Perform operation\n",
        "word_embed = embeddings_index[['female']] - \n",
        "              embeddings_index[['male']] + \n",
        "              embeddings_index[['programmer']]\n",
        "\n",
        "# Get closest word and similarity\n",
        "closest = find_closest(embeddings_index, \n",
        "                       word_embed, c('female', 'male', 'programmer'))\n",
        "\n",
        "# Print the closest word and similarity\n",
        "print(paste0(\"Closest word to female - male + programmer: \", closest[1]))\n",
        "print(paste0(\"Cosine similarity: \", closest[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xbYh3lCIlm-e",
        "outputId": "4a7ed3a1-de63-4ab2-bd4c-478f0769e892"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Closest word to female - male + programmer: animator\"\n",
            "[1] \"Cosine similarity: 0.639964123410331\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There seems to be a gender bias in the word embeddings which seem to be evident when looking at the cosine similarities. The word similarity is very less for female programmers, indicating that there are very less places where female programmers are mentioned in the web. Similarly the closest word to women doctor is nurse which indicates bias in women being doctors.  "
      ],
      "metadata": {
        "id": "qj91TQY4GTp2"
      }
    }
  ]
}